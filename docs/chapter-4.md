# Build a Large Language Model from Scratch

## Implementing a GPT model from scratch to generate text

### References

#### Transformers

- [The transformer architecture](https://www.youtube.com/watch?v=GhdB7UMtGqs) (2024) por Donato Capitella.

#### Shortcuts

When creating sequences that form our transformer blocks, we are adding shortcuts. To understand them, some literature about resudial networks might help. Here are some explainer videos:

- [Why residual connections work](https://www.youtube.com/watch?v=Gey9CG6R6w8) (2022) por DataMListic.
- [Residual networks and skip connections](https://www.youtube.com/watch?v=Q1JCrG1bJ-A) (2022) por Professor Bryce.

... and here are some relevant papers about the subject:

- [Deep residual learning for image recognition](https://arxiv.org/abs/1512.03385) (2015) por Kaiming He, Xiangyu Zhang, Shaoqing Ren y Jian Sun.
- [Visualizing the loss landscape of neural nets](https://arxiv.org/abs/1712.09913) (2017) por Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer y Tom Goldstein.
