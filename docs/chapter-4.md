# Build a Large Language Model from Scratch

## Implementing a GPT model from scratch to generate text

### References

#### Transformers

- [Visualizing transformers and attention](https://www.youtube.com/watch?v=KJtZARuO3JY) (2024) by Grant Sanderson.
- [Encoder vs decoder models](https://www.youtube.com/watch?v=XdGeVzDiYgg) (2024) by Emily McMilin.
- [The transformer architecture](https://www.youtube.com/watch?v=tstbZXNCfLY) (2021) by Sebastian Raschka.
- [The transformer architecture](https://www.youtube.com/watch?v=GhdB7UMtGqs) (2024) by Donato Capitella.

#### Shortcuts

When creating sequences that form our transformer blocks, we are adding shortcuts. To understand them, some literature about resudial networks might help. Here are some explainer videos:

- [Why residual connections work](https://www.youtube.com/watch?v=Gey9CG6R6w8) (2022) by DataMListic.
- [Residual networks and skip connections](https://www.youtube.com/watch?v=Q1JCrG1bJ-A) (2022) by Professor Bryce.

... and here are some relevant papers about the subject:

- [Deep residual learning for image recognition](https://arxiv.org/abs/1512.03385) (2015) by Kaiming He, Xiangyu Zhang, Shaoqing Ren and Jian Sun.
- [Visualizing the loss landscape of neural nets](https://arxiv.org/abs/1712.09913) (2017) by Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer and Tom Goldstein.
