# Build a Large Language Model from Scratch

## Implementing a GPT model from scratch to generate text

### References

#### Transformers

- [The transformer architecture](https://www.youtube.com/watch?v=GhdB7UMtGqs) (2024)

#### Shortcuts

When creating sequences that form our transformer blocks, we are adding shortcuts. To understand them, some literature about resudial networks might help. Here are some explainer videos:

- [Why residual connections work](https://www.youtube.com/watch?v=Gey9CG6R6w8) (2022)
- [Residual networks and skip connections](https://www.youtube.com/watch?v=Q1JCrG1bJ-A) (2022)

... and here are some relevant papers about the subject:

- [Deep residual learning for image recognition](https://arxiv.org/abs/1512.03385) (2015)
- [Visualizing the loss landscape of neural nets](https://arxiv.org/abs/1712.09913) (2017)
